{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 불러오는 파일 경로들은 사용중인 컴퓨터에 저장된 경로에 따라 수정이 필요합니다. \n",
    "# 크롤링 및 기타 자료 수집에 시간을 쓰지 않기 위해선 \"0전환.xlsx\" 와 \"kospiCode.xlsx\" 파일이 필요합니다. \n",
    "# (JTH_database for Python.zip 압축파일에 있음.)\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "import re\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import html5lib\n",
    "import datetime\n",
    "\n",
    "# 크롤링 함수 정의\n",
    "\n",
    "def Crawling(code):\n",
    "    \n",
    "    url = \"http://finance.naver.com/item/board.nhn?code={0}&page=1\".format(code)\n",
    "    HEADERS={'user-agent':('Mozilla/5.0 (Windows NT 6.1)' 'AppleWebKit/537.36 (KHTML, like Gecko)' 'Chrome/61.0.3163.100 Safari/537.36')}\n",
    "\n",
    "    r = requests.get(url, headers=HEADERS)\n",
    "    soup = bs(r.text,'html.parser').select(\"tbody\")[2]\n",
    "    try: \n",
    "        end_page=int(re.search('page=(.*?)\"',str(soup.select('.pgRR'))).group(1))\n",
    "    except: \n",
    "        end_page=1\n",
    "        \n",
    "    resultList=[]\n",
    "    for i in range(1,end_page+1):\n",
    "        url = \"http://finance.naver.com/item/board.nhn?code={0}&page={1}\".format(code, i)\n",
    "        r = requests.get(url)\n",
    "        soup = bs(r.text,'html.parser')\n",
    "        search_table = soup.findAll(\"span\", class_=\"gray03\")\n",
    "        list1=[date.text for date in search_table]\n",
    "        list2=[s for s in list1 if len(s) > 10]    \n",
    "        list3=[l[:10].replace('.','-') for l in list2]\n",
    "        for u in list3:        \n",
    "            resultList.append(u) \n",
    "        counts=dict(Counter(resultList))\n",
    "        data=pd.Series(counts,name=code)\n",
    "        data2=data.to_frame()\n",
    "        print(i)\n",
    "    return data2\n",
    "\n",
    "kospi=pd.read_excel('kospiCode.xlsx',header=None, dtype=object) # 경로 설정 및 코스피 종목 파일 필요\n",
    "kospiCode=kospi[0].tolist()\n",
    "\n",
    "# 크롤링 함수 실행코드 (한 번에 돌리면 오류 발생 확률이 매우 높으므로 나눠서 돌릴 필요 있음)\n",
    "\n",
    "appended_data=[]\n",
    "for i in kospiCode:\n",
    "    data=Crawling(i)\n",
    "    appended_data.append(data)\n",
    "    print(\"종목코드 \"+i+\" 리퀘스트완료...\")\n",
    "appended_data=pd.concat(appended_data, axis=1)\n",
    "print(\"표 통합완료\")\n",
    "\n",
    "appended_data = appended_data.fillna(0)\n",
    "appended_data.to_excel('0전환.xlsx')\n",
    "\n",
    "df = pd.read_excel('0전환.xlsx')\n",
    "\n",
    "result_dict = dict()\n",
    "for i in range(0,736):\n",
    "    company_name = df.columns[i]\n",
    "    result_dict[company_name] = [list(df[company_name].loc[lambda s: s>df[company_name].mean()+df[company_name].std()*-3].index),\\\n",
    "                                list(df[company_name].loc[lambda s: s>df[company_name].mean()+df[company_name].std()*-2].index),\\\n",
    "                                list(df[company_name].loc[lambda s: s>df[company_name].mean()+df[company_name].std()*2].index),\\\n",
    "                                list(df[company_name].loc[lambda s: s>df[company_name].mean()+df[company_name].std()*3].index)]\n",
    "\n",
    "new_df = pd.DataFrame(result_dict, index = [\"std_-3\",\"std_-2\",\"std_2\",\"std_3\"])\n",
    "\n",
    "new_df.to_excel(\"표준편차별정리테이블2.xlsx\")\n",
    "\n",
    "df = pd.read_excel(\"표준편차별정리테이블2.xlsx\")\n",
    "\n",
    "# 이 아래 부분은 '표준편차별정리테이블2.xlsx' 파일에서 std_3, 즉 평균게시글보다 3 표준편차 이상 게시글이 올라온 날짜만을 뽑아내어,\n",
    "# SAS 에서 돌릴 수 있도록 첫번째 열에는 종목코드, 두번째 열에는 해당 날짜가 들어가도록 엑셀을 만듭니다. \n",
    "# 표준편차 2 이상을 구하는 경우 아래 코드에서 \"std_3\" 라고 적힌 부분을 \"std_2\" 로 바꾸어 따로 저장했습니다. \n",
    "\n",
    "for i in df.columns:\n",
    "    dict1[i] = ast.literal_eval(df.loc[\"std_3\",i])\n",
    "\n",
    "df2= pd.DataFrame.from_dict(dict1, orient=\"index\")\n",
    "\n",
    "series1 = pd.Series()\n",
    "for i in range(736):\n",
    "    list1 = list(dict1.values())[i]\n",
    "    se1 = pd.Series(list1)\n",
    "    se1.index = [df.columns[i]]*len(list(dict1.values())[i])\n",
    "    series1 = series1.append(se1)\n",
    "    \n",
    "series1.to_excel(\"SAS용_std3.xlsx\")\n",
    "\n",
    "# 옵션!\n",
    "# 매일매일 코스피 전체를 크롤링해서, std3 이상 게시글이 올라온 코드를 메일로 전송하기(이후 윈도우 스케쥴러 적용 필요)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "def timer_crawling_first(code, num_page):\n",
    "        \n",
    "    url = \"http://finance.naver.com/item/board.nhn?code={0}&page={1}\".format(code, num_page)\n",
    "    HEADERS={'user-agent':('Mozilla/5.0 (Windows NT 6.1)' 'AppleWebKit/537.36 (KHTML, like Gecko)' 'Chrome/61.0.3163.100 Safari/537.36')}\n",
    "    r = requests.get(url, headers=HEADERS)\n",
    "    soup = bs(r.text,'html.parser').select('tbody')[0]\n",
    "    dates = soup.findAll(\"span\", class_=\"gray03\")\n",
    "    now = datetime.datetime.now()\n",
    "    p = re.compile(str(now)[:10].replace('-','.'))\n",
    "    todaylist = p.findall(str(dates))\n",
    "    length = len(todaylist)\n",
    "    return length\n",
    "\n",
    "def additional(code, num_page):\n",
    "    \n",
    "    if timer_crawling_first(code, num_page) < 20 :\n",
    "        return timer_crawling_first(code, num_page)\n",
    "    else: \n",
    "        return 20 + additional(code, num_page+1)\n",
    "\n",
    "kospi = pd.read_excel('C:\\\\Users\\\\acous\\\\PythonCoding\\\\Projects\\\\kospiCode.xlsx',header=None, dtype=object)\n",
    "kospiCode = kospi[0].tolist() \n",
    "\n",
    "resultdict = {}\n",
    "for i in kospiCode:\n",
    "    result = additional(i,1)\n",
    "    resultdict[i] = result\n",
    "    \n",
    "df=pd.read_excel('C:\\\\Users\\\\acous\\\\PythonCoding\\\\Projects\\\\0전환.xlsx')\n",
    "    \n",
    "mean_plus_std3 = {}\n",
    "for i in df.columns:\n",
    "    mean_plus_std3[i] = df[i].mean()+df[i].std()*3\n",
    "    \n",
    "def compare(countdict,meandict):\n",
    "    return [i for i in df.columns if countdict[i] > meandict[i]]   \n",
    "\n",
    "final = compare(resultdict,mean_plus_std3)\n",
    "text_contents = str(final)\n",
    "\n",
    "# 메일보내는용\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "def sendMail(from_email,to_email,text_cont,html_cont):\n",
    "\n",
    "    message = MIMEMultipart('alternative')\n",
    "    message['Subject'] = \"오늘의 종목: 표준편차3 이상 게시글 개수\"\n",
    "    message['From'] = from_email\n",
    "    message['To'] = to_email\n",
    "\n",
    "    part1 = MIMEText(text_cont,'plain')\n",
    "    part2 = MIMEText(html_cont,'html')\n",
    "\n",
    "    message.attach(part1)\n",
    "    message.attach(part2)\n",
    "\n",
    "\n",
    "    smtp_server = smtplib.SMTP('smtp.gmail.com:587')\n",
    "    smtp_server.starttls()\n",
    "    smtp_server.login(\"개인 이메일 주소 입력\", \"개인 비밀번호 입력\")\n",
    "    smtp_server.sendmail(from_email,to_email,message.as_string())\n",
    "    smtp_server.quit()      \n",
    "    \n",
    "html_contents = str(final)\n",
    "    \n",
    "try: \n",
    "    sendMail(\"보내는 메일 주소\",\"받는 메일 주소(보내는 메일과 받는 메일 같게도 가능)\",text_contents,html_contents)\n",
    "except: print('문제가 있다')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
